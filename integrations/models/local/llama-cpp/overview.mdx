---
title: LlamaCpp
sidebarTitle: 概览
description: Learn how to use LlamaCpp with Agno.
---

<Badge icon="code-branch" color="orange">
    <Tooltip tip="Introduced in v2.0.7" cta="查看 release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.0.7">v2.0.7</Tooltip>
</Badge>

Run Large Language Models 本地ly with LLaMA CPP

[LlamaCpp](https://github.com/ggerganov/llama.cpp) is a powerful tool for running large language 模型s 本地ly with efficient 推理. LlamaCpp 支持 multiple open-source 模型s and provides an OpenAI-兼容 API server.

LlamaCpp 支持 a wide variety of 模型s in GGML format. You can find 模型s on HuggingFace, including the default `ggml-org/gpt-oss-20b-GGUF` used in the examples below.

We recommend experimenting to find the best 模型 for your use case. Here are some popular 模型 recommendations:

### Google Gemma Models

- `google/gemma-2b-it-GGUF` - Lightweight 2B parameter 模型, great for resource-constrained environments
- `google/gemma-7b-it-GGUF` - Balanced 7B 模型 with strong performance for general tasks
- `ggml-org/gemma-3-1b-it-GGUF` - Latest Gemma 3 series, efficient for everyday use

### Meta Llama Models

- `Meta-Llama-3-8B-Instruct` - Popular 8B parameter 模型 with excellent instruction 以下
- `Meta-Llama-3.1-8B-Instruct` - Enhanced version with improved 功能 and 128K context
- `Meta-Llama-3.2-3B-Instruct` - Compact 3B 模型 for faster 推理

### Default Options

- `ggml-org/gpt-oss-20b-GGUF` - Default 模型 for general use cases
- Models with different quantizations (Q4_K_M, Q8_0, etc.) for different speed/quality tradeoffs
- Choose 模型s based on your hardware constraints and performance requirements

## Set up LlamaCpp

### 安装 LlamaCpp

First, install LlamaCpp 以下 the [official installation guide](https://github.com/ggerganov/llama.cpp):

```bash install
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
make
```

Or using package managers:

```bash brew install
# macOS with Homebrew
brew install llama.cpp
```

### Download a Model

Download a 模型 in GGUF format 以下 the [llama.cpp 模型 download guide](https://github.com/ggerganov/llama.cpp#obtaining-and-using-the-facebook-llama-2-模型). For the examples below, we use `ggml-org/gpt-oss-20b-GGUF`.

### Start the Server

Start the LlamaCpp server with your 模型:

```bash start server
llama-server -hf ggml-org/gpt-oss-20b-GGUF --ctx-size 0 --jinja -ub 2048 -b 2048
```

This starts the server at `http://127.0.0.1:8080` with an OpenAI Chat 兼容 endpoints

## Example

After starting the LlamaCpp server, use the `LlamaCpp` 模型 class to access it:

<CodeGroup>

```python agent.py
from agno.agent import Agent
from agno.models.llama_cpp import LlamaCpp

agent = Agent(
    model=LlamaCpp(id="ggml-org/gpt-oss-20b-GGUF"),
    markdown=True
)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story.")
```

</CodeGroup>

## Configuration

The `LlamaCpp` 模型 支持 customizing the server URL and 模型 ID:

<CodeGroup>

```python custom_config.py
from agno.agent import Agent
from agno.models.llama_cpp import LlamaCpp

# Custom server configuration
agent = Agent(
    model=LlamaCpp(
        id="your-custom-model",
        base_url="http://localhost:8080/v1",  # Custom server URL
    ),
    markdown=True
)
```

</CodeGroup>

<Note> 查看 更多功能 examples [here](/integrations/models/local/llama-cpp/usage/basic). </Note>

## 参数

| Parameter      | Type                       | Default                        | Description                                                                                                          |
| -------------- | -------------------------- | ------------------------------ | -------------------------------------------------------------------------------------------------------------------- |
| `id`           | `str`                      | `"llama-cpp"`                  | The identifier for the Llama.cpp 模型                                                                              |
| `name`         | `str`                      | `"LlamaCpp"`                   | The name of the 模型                                                                                                |
| `提供商`     | `str`                      | `"LlamaCpp"`                   | The 提供商 of the 模型                                                                                            |
| `base_url`     | `str`                      | `"http://本地host:8080"`      | The base URL for the Llama.cpp server                                                                               |
| `api_key`      | `Optional[str]`            | `None`                         | The API key (usually not needed for 本地 Llama.cpp)                                                               |
| `chat_format`  | `Optional[str]`            | `None`                         | The chat format to use (e.g., "chatml", "llama-2", "alpaca")                                                      |
| `n_ctx`        | `Optional[int]`            | `None`                         | The context window size                                                                                              |
| `temperature`  | `Optional[float]`          | `None`                         | Sampling temperature (0.0 to 2.0)                                                                                   |
| `top_p`        | `Optional[float]`          | `None`                         | Top-p sampling parameter                                                                                             |
| `top_k`        | `Optional[int]`            | `None`                         | Top-k sampling parameter                                                                                             |

`LlamaCpp` is a subclass of the [OpenAILike](/integrations/models/openai-like) class and has access to the same params.

## Server Configuration

The LlamaCpp server 支持 many configuration options:

### Common Server Options

- `--ctx-size`: Context size (0 for unlimited)
- `--batch-size`, `-b`: Batch size for prompt processing
- `--ubatch-size`, `-ub`: Physical batch size for prompt processing
- `--threads`, `-t`: Number of threads to use
- `--host`: IP address to listen on (default: 127.0.0.1)
- `--port`: Port to listen on (default: 8080)

### Model Options

- `--模型`, `-m`: Model file path
- `--hf-repo`: HuggingFace 模型 repository
- `--jinja`: Use Jinja templating for chat formatting

For a complete list of server options, run `llama-server --help`.

## Performance Optimization

### Hardware Acceleration

LlamaCpp 支持 various acceleration backends:

```bash gpu acceleration
# NVIDIA GPU (CUDA)
make LLAMA_CUDA=1

# Apple Metal (macOS)
make LLAMA_METAL=1

# OpenCL
make LLAMA_CLBLAST=1
```

### Model Quantization

Use quantized 模型s for better performance:

- `Q4_K_M`: Balanced size and quality
- `Q8_0`: Higher quality, larger size
- `Q2_K`: Smallest size, lower quality

## Troubleshooting

### Server Connection Issues

Ensure the LlamaCpp server is running and accessible:

```bash check server
curl http://127.0.0.1:8080/v1/models
```

### Model Loading Problems

- Verify the 模型 file exists and is in GGML format
- Check available 记忆 for large 模型s
- Ensure the 模型 is 兼容 with your LlamaCpp version

### Performance Issues

- Adjust batch sizes (`-b`, `-ub`) based on your hardware
- Use GPU acceleration if available
- Consider using quantized 模型s for faster 推理
