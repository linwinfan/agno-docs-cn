---
title: vLLM
sidebarTitle: 概览
---

[vLLM](https://docs.vllm.ai/en/latest/) is a fast and easy-to-use library for LLM 推理 and serving, designed for 高吞吐量 and 记忆-efficient LLM serving.

## Prerequisites

安装 vLLM and start serving a 模型:

```bash install vLLM
pip install vllm
```

```bash start vLLM server
vllm serve Qwen/Qwen2.5-7B-Instruct \
    --enable-auto-tool-choice \
    --tool-call-parser hermes \
    --dtype float16 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.9
```

This spins up the vLLM server with an OpenAI-兼容 API.

<Note>The default vLLM server URL is `http://本地host:8000/`</Note>

## Example

Basic Agent

<CodeGroup>

```python agent.py
from agno.agent import Agent
from agno.models.vllm import VLLM

agent = Agent(
    model=VLLM(
        id="meta-llama/Llama-3.1-8B-Instruct",
        base_url="http://localhost:8000/",
    ),
    markdown=True
)

agent.print_response("Share a 2 sentence horror story.")
```

</CodeGroup>

## Advanced 使用方法

### With Tools

vLLM 模型s work seamlessly with Agno tools:

```python with_tools.py
from agno.agent import Agent
from agno.models.vllm import VLLM
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=VLLM(id="meta-llama/Llama-3.1-8B-Instruct"),
    tools=[DuckDuckGoTools()],
    markdown=True
)

agent.print_response("What's the latest news about AI?")
```

<Note> 查看 更多功能 examples [here](/integrations/models/local/vllm/usage/basic-stream). </Note>

For the full list of supported 模型s, see the [vLLM 文档ation](https://docs.vllm.ai/en/latest/models/supported_模型s.html).

## 参数

| Parameter    | Type               | Default                        | Description                                                           |
| ------------ | ------------------ | ------------------------------ | --------------------------------------------------------------------- |
| `id`         | `str`              | `"microsoft/DialoGPT-medium"`  | The id of the 模型 to use with vLLM                                 |
| `name`       | `str`              | `"vLLM"`                       | The name of the 模型                                                 |
| `提供商`   | `str`              | `"vLLM"`                       | The 提供商 of the 模型                                             |
| `api_key`    | `Optional[str]`    | `None`                         | The API key (usually not needed for 本地 vLLM)                      |
| `base_url`   | `str`              | `"http://本地host:8000/v1"`  | The base URL for the vLLM server                                     |

`VLLM` is a subclass of the [Model](/reference/models/模型) class and has access to the same params.
