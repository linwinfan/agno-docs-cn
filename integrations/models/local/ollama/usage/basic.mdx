---
title: Basic
---

## Code

```python cookbook/11_models/ollama/basic.py
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="llama3.1:8b"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")

```

## 使用方法

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="安装 Ollama">
    Follow the [Ollama installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) 并运行:
    ```bash
    ollama pull llama3.1:8b
    ```
  </Step>

  <Step title="安装 libraries">
    ```bash
    pip install -U ollama agno
    ```
  </Step>

  <Step title="Run Agent">
    <CodeGroup>
    ```bash Mac
    python cookbook/11_models/ollama/basic.py
    ```

    ```bash Windows
    python cookbook/11_models/ollama/basic.py
    ```
    </CodeGroup>
  </Step>
</Steps>

## Cloud Alter原生

For easier setup without 本地 installation, you can use [Ollama Cloud](/integrations/models/local/ollama/usage/云) with your API key:

```python
from agno.agent import Agent
from agno.models.ollama import Ollama

# No local setup required - just set OLLAMA_API_KEY
agent = Agent(model=Ollama(id="gpt-oss:120b-cloud", host="https://ollama.com"))
agent.print_response("Share a 2 sentence horror story")
```
