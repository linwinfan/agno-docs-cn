---
title: LiteLLM
sidebarTitle: 概览
description: Integrate LiteLLM with Agno for a 统一 LLM experience.
---

[LiteLLM](https://docs.litellm.ai/docs/) provides a 统一 interface for various LLM 提供商s, allowing you to use different 模型s with the same code.

Agno integrates with LiteLLM in two ways:

1. **Direct SDK 集成** - Using the LiteLLM Python SDK
2. **Proxy Server 集成** - Using LiteLLM as an OpenAI-兼容 proxy

## Prerequisites

For both 集成 methods, you'll need:

```shell
# Install required packages
pip install agno litellm
```

Set up your API key:
Regardless of the 模型 used(OpenAI, Hugging Face, or XAI) the API key is referenced as `LITELLM_API_KEY`.

```shell
export LITELLM_API_KEY=your_api_key_here
```

## SDK Integration

The `LiteLLM` class provides direct 集成 with the LiteLLM Python SDK.

### Basic 使用方法

```python
from agno.agent import Agent
from agno.models.litellm import LiteLLM

# Create an agent with GPT-4o
agent = Agent(
    model=LiteLLM(
        id="gpt-5-mini",  # Model ID to use
        name="LiteLLM",  # Optional display name
    ),
    markdown=True,
)

# Get a response
agent.print_response("Share a 2 sentence horror story")
```

### Using Hugging Face Models

LiteLLM can also work with Hugging Face 模型s:

```python
from agno.agent import Agent
from agno.models.litellm import LiteLLM

agent = Agent(
    model=LiteLLM(
        id="huggingface/mistralai/Mistral-7B-Instruct-v0.2",
        top_p=0.95,
    ),
    markdown=True,
)

agent.print_response("What's happening in France?")
```

### Configuration Options

The `LiteLLM` class accepts the 以下 parameters:

| Parameter        | Type                     | Description                                                                           | Default   |
| ---------------- | ------------------------ | ------------------------------------------------------------------------------------- | --------- |
| `id`             | str                      | Model identifier (e.g., "gpt-5-mini" or "huggingface/mistralai/Mistral-7B-Instruct-v0.2") | "gpt-5-mini"  |
| `name`           | str                      | Display name for the 模型                                                            | "LiteLLM" |
| `提供商`       | str                      | Provider name                                                                         | "LiteLLM" |
| `api_key`        | Optional[str]            | API key (falls back to LITELLM_API_KEY environment variable)                          | None      |
| `api_base`       | Optional[str]            | Base URL for API requests                                                             | None      |
| `max_tokens`     | Optional[int]            | Maximum tokens in the response                                                        | None      |
| `temperature`    | float                    | Sampling temperature                                                                  | 0.7       |
| `top_p`          | float                    | Top-p sampling value                                                                  | 1.0       |
| `request_params` | Optional[Dict[str, Any]] | Additional request parameters                                                         | None      |

### Examples

<Note> 查看 更多功能 examples [here](/integrations/models/gateways/litellm/usage/basic-stream). </Note>
